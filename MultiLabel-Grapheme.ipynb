{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from model.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "from PIL import Image\n",
    "import pandas as pd \n",
    "\n",
    "class MultiOutputModel(nn.Module):\n",
    "    def __init__(self, n_grapheme_classes, n_vowel_classes, n_consonant_classes):\n",
    "        super().__init__()\n",
    "        self.base_model = models.mobilenet_v2().features  # take the model without classifier\n",
    "        self.base_model[0][0] = nn.Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
    "\n",
    "        last_channel = models.mobilenet_v2().last_channel  # size of the layer before classifier\n",
    "\n",
    "        # the input for the classifier should be two-dimensional, but we will have\n",
    "        # [batch_size, channels, width, height]\n",
    "        # so, let's do the spatial averaging: reduce width and height to 1\n",
    "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "\n",
    "        # create separate classifiers for our outputs\n",
    "        self.grapheme = nn.Sequential(\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(in_features=last_channel, out_features=n_grapheme_classes)\n",
    "        )\n",
    "        self.vowel = nn.Sequential(\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(in_features=last_channel, out_features=n_vowel_classes)\n",
    "        )\n",
    "        self.consonant = nn.Sequential(\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(in_features=last_channel, out_features=n_consonant_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.base_model(x)\n",
    "        x = self.pool(x)\n",
    "\n",
    "        # reshape from [batch, channels, 1, 1] to [batch, channels] to put it into classifier\n",
    "        x = torch.flatten(x, 1)\n",
    "\n",
    "        return {\n",
    "            'grapheme': self.grapheme(x),\n",
    "            'vowel': self.vowel(x),\n",
    "            'consonant': self.consonant(x)\n",
    "        }\n",
    "\n",
    "    def get_loss(self, net_output, ground_truth):\n",
    "        color_loss = F.cross_entropy(net_output['grapheme'], ground_truth['grapheme_labels'])\n",
    "        gender_loss = F.cross_entropy(net_output['vowel'], ground_truth['vowel_labels'])\n",
    "        article_loss = F.cross_entropy(net_output['consonant'], ground_truth['consonant_labels'])\n",
    "        loss = color_loss + gender_loss + article_loss\n",
    "        return loss, {'grapheme': color_loss, 'vowel': gender_loss, 'consonant': article_loss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BengaliDatasetMultiClass(Dataset):\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.label_df = pd.read_csv(csv_file)\n",
    "        self.label_df = self.label_df[['image_id','grapheme_root',\n",
    "                             'vowel_diacritic','consonant_diacritic',\n",
    "                             'label','grapheme','textlabel']]\n",
    "        \n",
    "        self.root_dir = root_dir \n",
    "        self.transform = transform \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.label_df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "            \n",
    "        img_name = os.path.join(self.root_dir,\n",
    "                                self.label_df.iloc[idx, 0] + '.png')\n",
    "        image = Image.open(img_name).convert('L')\n",
    "\n",
    "        label = tuple(self.label_df.iloc[idx, 1:4])\n",
    "        label = torch.tensor(label)\n",
    "        textlabel = self.label_df.iloc[idx, -1]  \n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        sample = {\n",
    "            \"image\": image,\n",
    "            \"labels\": {\n",
    "                \"grapheme_labels\": label[0],\n",
    "                \"vowel_labels\": label[1],\n",
    "                \"consonant_labels\": label[2]\n",
    "            }\n",
    "        }\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=4\n",
    "transform=transforms.Compose([\n",
    "    transforms.Resize(28),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "trainset = BengaliDatasetMultiClass(\"data/train.csv\",\"data/trainsplit\", transform)\n",
    "testset = BengaliDatasetMultiClass(\"data/test.csv\",\"data/testsplit\", transform)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                         shuffle=True, num_workers=0)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = next(iter(trainloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultiOutputModel(168, 11,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 168])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net(samples[\"image\"])[\"grapheme\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score\n",
    "\n",
    "def calculate_metrics(output, target):\n",
    "    _, predicted_color = output['grapheme'].cpu().max(1)\n",
    "    gt_color = target['grapheme_labels'].cpu()\n",
    "\n",
    "    _, predicted_gender = output['vowel'].cpu().max(1)\n",
    "    gt_gender = target['vowel_labels'].cpu()\n",
    "\n",
    "    _, predicted_article = output['consonant'].cpu().max(1)\n",
    "    gt_article = target['consonant_labels'].cpu()\n",
    "\n",
    "    with warnings.catch_warnings():  # sklearn may produce a warning when processing zero row in confusion matrix\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        accuracy_color = accuracy_score(y_true=gt_color.numpy(), y_pred=predicted_color.numpy())\n",
    "        accuracy_gender = accuracy_score(y_true=gt_gender.numpy(), y_pred=predicted_gender.numpy())\n",
    "        accuracy_article = accuracy_score(y_true=gt_article.numpy(), y_pred=predicted_article.numpy())\n",
    "\n",
    "    return accuracy_color, accuracy_gender, accuracy_article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from train.py\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "def get_cur_time():\n",
    "    return datetime.strftime(datetime.now(), '%Y-%m-%d_%H-%M')\n",
    "\n",
    "\n",
    "def checkpoint_save(model, name, epoch):\n",
    "    f = os.path.join(name, 'checkpoint-{:06d}.pth'.format(epoch))\n",
    "    torch.save(model.state_dict(), f)\n",
    "    print('Saved checkpoint:', f)\n",
    "\n",
    "    return f\n",
    "    \n",
    "\n",
    "def train(start_epoch=1, N_epochs=1, batch_size=16, num_workers=8):\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "    for epoch in range(start_epoch, N_epochs + 1):\n",
    "        total_loss = 0\n",
    "        accuracy_color = 0\n",
    "        accuracy_gender = 0\n",
    "        accuracy_article = 0\n",
    "\n",
    "        for batch in trainloader:\n",
    "            print(\"hello\")\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            img = batch['image']\n",
    "            target_labels = batch['labels']\n",
    "            target_labels = {t: target_labels[t].to(device) for t in target_labels}\n",
    "            output = model(img.to(device))\n",
    "\n",
    "            loss_train, losses_train = model.get_loss(output, target_labels)\n",
    "            total_loss += loss_train.item()\n",
    "            batch_accuracy_color, batch_accuracy_gender, batch_accuracy_article = \\\n",
    "                calculate_metrics(output, target_labels)\n",
    "\n",
    "            accuracy_color += batch_accuracy_color\n",
    "            accuracy_gender += batch_accuracy_gender\n",
    "            accuracy_article += batch_accuracy_article\n",
    "\n",
    "            loss_train.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        print(\"epoch {:4d}, loss: {:.4f}, color: {:.4f}, gender: {:.4f}, article: {:.4f}\".format(\n",
    "            epoch,\n",
    "            total_loss / n_train_samples,\n",
    "            accuracy_color / n_train_samples,\n",
    "            accuracy_gender / n_train_samples,\n",
    "            accuracy_article / n_train_samples))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n",
      "{'grapheme_labels': tensor([ 86,  57, 160, 113]), 'vowel_labels': tensor([0, 2, 2, 1]), 'consonant_labels': tensor([0, 0, 0, 4])}\n",
      "hello\n",
      "{'grapheme_labels': tensor([113, 148, 133, 150]), 'vowel_labels': tensor([5, 7, 2, 4]), 'consonant_labels': tensor([2, 0, 4, 0])}\n",
      "hello\n",
      "{'grapheme_labels': tensor([ 79, 142,  82,  13]), 'vowel_labels': tensor([3, 2, 4, 1]), 'consonant_labels': tensor([0, 0, 0, 5])}\n",
      "hello\n",
      "{'grapheme_labels': tensor([81, 72, 71, 56]), 'vowel_labels': tensor([7, 8, 0, 2]), 'consonant_labels': tensor([2, 0, 3, 5])}\n",
      "hello\n",
      "{'grapheme_labels': tensor([ 42,  64,  96, 134]), 'vowel_labels': tensor([ 4,  8, 10,  4]), 'consonant_labels': tensor([1, 5, 1, 0])}\n",
      "hello\n",
      "{'grapheme_labels': tensor([155, 129, 149,  44]), 'vowel_labels': tensor([7, 3, 2, 3]), 'consonant_labels': tensor([0, 0, 5, 0])}\n",
      "hello\n",
      "{'grapheme_labels': tensor([72, 57, 88, 10]), 'vowel_labels': tensor([3, 1, 0, 0]), 'consonant_labels': tensor([2, 0, 0, 0])}\n",
      "hello\n",
      "{'grapheme_labels': tensor([79, 13, 52, 59]), 'vowel_labels': tensor([7, 9, 4, 9]), 'consonant_labels': tensor([0, 2, 0, 0])}\n",
      "hello\n",
      "{'grapheme_labels': tensor([18, 25, 63, 31]), 'vowel_labels': tensor([0, 7, 0, 0]), 'consonant_labels': tensor([0, 0, 0, 0])}\n",
      "hello\n",
      "{'grapheme_labels': tensor([ 89,  96, 122, 129]), 'vowel_labels': tensor([7, 8, 5, 0]), 'consonant_labels': tensor([0, 0, 0, 0])}\n",
      "hello\n",
      "{'grapheme_labels': tensor([ 91,  84, 159,  89]), 'vowel_labels': tensor([0, 7, 3, 3]), 'consonant_labels': tensor([5, 0, 0, 5])}\n",
      "hello\n",
      "{'grapheme_labels': tensor([38, 57, 23, 37]), 'vowel_labels': tensor([3, 3, 7, 2]), 'consonant_labels': tensor([0, 0, 2, 0])}\n",
      "hello\n",
      "{'grapheme_labels': tensor([ 81,  62,  59, 115]), 'vowel_labels': tensor([0, 2, 9, 7]), 'consonant_labels': tensor([2, 0, 0, 2])}\n",
      "hello\n",
      "{'grapheme_labels': tensor([133,  42, 107,  57]), 'vowel_labels': tensor([7, 1, 2, 0]), 'consonant_labels': tensor([4, 0, 5, 0])}\n",
      "hello\n",
      "{'grapheme_labels': tensor([ 23, 109, 148,  43]), 'vowel_labels': tensor([4, 2, 1, 3]), 'consonant_labels': tensor([5, 0, 4, 2])}\n",
      "hello\n",
      "{'grapheme_labels': tensor([ 50,  81,  47, 162]), 'vowel_labels': tensor([4, 5, 2, 0]), 'consonant_labels': tensor([0, 0, 0, 0])}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-110-2da0ffaf5447>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-109-2ae730a81dd1>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(start_epoch, N_epochs, batch_size, num_workers)\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0maccuracy_article\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbatch_accuracy_article\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m             \u001b[0mloss_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/assessment/2020/STAT4402/statenv/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \"\"\"\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/assessment/2020/STAT4402/statenv/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    125\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    126\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
