{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# from model.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "from PIL import Image\n",
    "import pandas as pd \n",
    "import os \n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "class MultiOutputModel(nn.Module):\n",
    "    def __init__(self, n_grapheme_classes, n_vowel_classes, n_consonant_classes):\n",
    "        super().__init__()\n",
    "        self.base_model = models.mobilenet_v2().features  # take the model without classifier\n",
    "        self.base_model[0][0] = nn.Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
    "\n",
    "        last_channel = models.mobilenet_v2().last_channel  # size of the layer before classifier\n",
    "\n",
    "        # the input for the classifier should be two-dimensional, but we will have\n",
    "        # [batch_size, channels, width, height]\n",
    "        # so, let's do the spatial averaging: reduce width and height to 1\n",
    "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "\n",
    "        # create separate classifiers for our outputs\n",
    "        self.grapheme = nn.Sequential(\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(in_features=last_channel, out_features=n_grapheme_classes)\n",
    "        )\n",
    "        self.vowel = nn.Sequential(\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(in_features=last_channel, out_features=n_vowel_classes)\n",
    "        )\n",
    "        self.consonant = nn.Sequential(\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(in_features=last_channel, out_features=n_consonant_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.base_model(x)\n",
    "        x = self.pool(x)\n",
    "\n",
    "        # reshape from [batch, channels, 1, 1] to [batch, channels] to put it into classifier\n",
    "        x = torch.flatten(x, 1)\n",
    "\n",
    "        return {\n",
    "            'grapheme': self.grapheme(x),\n",
    "            'vowel': self.vowel(x),\n",
    "            'consonant': self.consonant(x)\n",
    "        }\n",
    "\n",
    "    def get_loss(self, net_output, ground_truth):\n",
    "        color_loss = F.cross_entropy(net_output['grapheme'], ground_truth['grapheme_labels'])\n",
    "        gender_loss = F.cross_entropy(net_output['vowel'], ground_truth['vowel_labels'])\n",
    "        article_loss = F.cross_entropy(net_output['consonant'], ground_truth['consonant_labels'])\n",
    "        loss = color_loss + gender_loss + article_loss\n",
    "        return loss, {'grapheme': color_loss, 'vowel': gender_loss, 'consonant': article_loss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BengaliDatasetMultiClass(Dataset):\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.label_df = pd.read_csv(csv_file)\n",
    "        self.label_df = self.label_df[['image_id','grapheme_root',\n",
    "                             'vowel_diacritic','consonant_diacritic',\n",
    "                             'label','grapheme','textlabel']]\n",
    "        \n",
    "        self.root_dir = root_dir \n",
    "        self.transform = transform \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.label_df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "            \n",
    "        img_name = os.path.join(self.root_dir,\n",
    "                                self.label_df.iloc[idx, 0] + '.png')\n",
    "        image = Image.open(img_name).convert('L')\n",
    "\n",
    "        label = tuple(self.label_df.iloc[idx, 1:])\n",
    "        textlabel = self.label_df.iloc[idx, -1]  \n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        sample = {\n",
    "            \"image\": image,\n",
    "            \"labels\": {\n",
    "                \"grapheme_labels\": label[0],\n",
    "                \"vowel_labels\": label[1],\n",
    "                \"consonant_labels\": label[2],\n",
    "            },\n",
    "            \"human_labels\":{\n",
    "                \"typeface\":label[4],\n",
    "                \"stringlabel\":label[5]\n",
    "            }\n",
    "        }\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=128\n",
    "num_workers = 16\n",
    "\n",
    "transform=transforms.Compose([\n",
    "    transforms.Resize(28),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "trainset = BengaliDatasetMultiClass(\"data/train.csv\",\"data/trainsplit\", transform)\n",
    "testset = BengaliDatasetMultiClass(\"data/test.csv\",\"data/testsplit\", transform)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                         shuffle=True, num_workers=num_workers)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=True, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = next(iter(trainloader))\n",
    "model = MultiOutputModel(168, 11, 7).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiOutputModel(\n",
       "  (base_model): Sequential(\n",
       "    (0): ConvBNReLU(\n",
       "      (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU6(inplace=True)\n",
       "    )\n",
       "    (1): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (2): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
       "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (3): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
       "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (4): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)\n",
       "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (5): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (6): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (7): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (8): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (9): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (10): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (11): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (12): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (13): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (14): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (15): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (16): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (17): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (18): ConvBNReLU(\n",
       "      (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU6(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (pool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (grapheme): Sequential(\n",
       "    (0): Dropout(p=0.2, inplace=False)\n",
       "    (1): Linear(in_features=1280, out_features=168, bias=True)\n",
       "  )\n",
       "  (vowel): Sequential(\n",
       "    (0): Dropout(p=0.2, inplace=False)\n",
       "    (1): Linear(in_features=1280, out_features=11, bias=True)\n",
       "  )\n",
       "  (consonant): Sequential(\n",
       "    (0): Dropout(p=0.2, inplace=False)\n",
       "    (1): Linear(in_features=1280, out_features=7, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score\n",
    "\n",
    "def calculate_metrics(output, target):\n",
    "    _, predicted_color = output['grapheme'].cpu().max(1)\n",
    "    gt_color = target['grapheme_labels'].cpu()\n",
    "\n",
    "    _, predicted_gender = output['vowel'].cpu().max(1)\n",
    "    gt_gender = target['vowel_labels'].cpu()\n",
    "\n",
    "    _, predicted_article = output['consonant'].cpu().max(1)\n",
    "    gt_article = target['consonant_labels'].cpu()\n",
    "\n",
    "    with warnings.catch_warnings():  # sklearn may produce a warning when processing zero row in confusion matrix\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        accuracy_color = accuracy_score(y_true=gt_color.numpy(), y_pred=predicted_color.numpy())\n",
    "        accuracy_gender = accuracy_score(y_true=gt_gender.numpy(), y_pred=predicted_gender.numpy())\n",
    "        accuracy_article = accuracy_score(y_true=gt_article.numpy(), y_pred=predicted_article.numpy())\n",
    "\n",
    "    return accuracy_color, accuracy_gender, accuracy_article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from train.py\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "# from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "def get_cur_time():\n",
    "    return datetime.strftime(datetime.now(), '%Y-%m-%d_%H-%M')\n",
    "\n",
    "\n",
    "def checkpoint_save(model, name, epoch):\n",
    "    f = os.path.join(name, 'checkpoint-{:06d}.pth'.format(epoch))\n",
    "    torch.save(model.state_dict(), f)\n",
    "    print('Saved checkpoint:', f)\n",
    "\n",
    "    return f\n",
    "    \n",
    "\n",
    "def train(start_epoch=1, N_epochs=20, log_interval=100):\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "\n",
    "    for epoch in range(start_epoch, N_epochs + 1):\n",
    "        total_loss = 0\n",
    "        accuracy_color = 0\n",
    "        accuracy_gender = 0\n",
    "        accuracy_article = 0\n",
    "\n",
    "        for i, batch in enumerate(trainloader):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            img = batch['image']\n",
    "            target_labels = batch['labels']\n",
    "            target_labels = {t: target_labels[t].to(device) for t in target_labels}\n",
    "            output = model(img.to(device))\n",
    "\n",
    "            loss_train, losses_train = model.get_loss(output, target_labels)\n",
    "            total_loss += loss_train.item()\n",
    "            batch_accuracy_color, batch_accuracy_gender, batch_accuracy_article = \\\n",
    "                calculate_metrics(output, target_labels)\n",
    "\n",
    "            accuracy_color += batch_accuracy_color\n",
    "            accuracy_gender += batch_accuracy_gender\n",
    "            accuracy_article += batch_accuracy_article\n",
    "\n",
    "            loss_train.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            if (i+1)%log_interval == 0: \n",
    "                print(\"epoch {:4d}, loss: {:.4f}, grapheme: {:.4f}, vowel: {:.4f}, consonant: {:.4f}\".format(\n",
    "                    epoch,\n",
    "                    total_loss / (log_interval*batch_size),\n",
    "                    accuracy_color / (log_interval*batch_size),\n",
    "                    accuracy_gender / (log_interval*batch_size),\n",
    "                    accuracy_article / (log_interval*batch_size)))\n",
    "                \n",
    "                # Reset stats \n",
    "                total_loss = 0\n",
    "                accuracy_color = 0\n",
    "                accuracy_gender = 0\n",
    "                accuracy_article = 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch    1, loss: 0.0649, grapheme: 0.0002, vowel: 0.0017, consonant: 0.0048\n",
      "epoch    1, loss: 0.0606, grapheme: 0.0002, vowel: 0.0024, consonant: 0.0049\n",
      "epoch    1, loss: 0.0565, grapheme: 0.0003, vowel: 0.0033, consonant: 0.0050\n",
      "epoch    1, loss: 0.0539, grapheme: 0.0003, vowel: 0.0039, consonant: 0.0051\n",
      "epoch    1, loss: 0.0512, grapheme: 0.0004, vowel: 0.0044, consonant: 0.0052\n",
      "epoch    1, loss: 0.0490, grapheme: 0.0004, vowel: 0.0047, consonant: 0.0054\n",
      "epoch    1, loss: 0.0471, grapheme: 0.0005, vowel: 0.0050, consonant: 0.0055\n",
      "epoch    1, loss: 0.0451, grapheme: 0.0005, vowel: 0.0053, consonant: 0.0057\n",
      "epoch    1, loss: 0.0430, grapheme: 0.0006, vowel: 0.0055, consonant: 0.0058\n",
      "epoch    1, loss: 0.0413, grapheme: 0.0007, vowel: 0.0058, consonant: 0.0059\n",
      "epoch    2, loss: 0.0390, grapheme: 0.0008, vowel: 0.0060, consonant: 0.0060\n",
      "epoch    2, loss: 0.0374, grapheme: 0.0009, vowel: 0.0061, consonant: 0.0062\n",
      "epoch    2, loss: 0.0362, grapheme: 0.0011, vowel: 0.0062, consonant: 0.0062\n",
      "epoch    2, loss: 0.0353, grapheme: 0.0011, vowel: 0.0062, consonant: 0.0062\n",
      "epoch    2, loss: 0.0343, grapheme: 0.0013, vowel: 0.0063, consonant: 0.0063\n",
      "epoch    2, loss: 0.0332, grapheme: 0.0014, vowel: 0.0064, consonant: 0.0063\n",
      "epoch    2, loss: 0.0325, grapheme: 0.0015, vowel: 0.0064, consonant: 0.0064\n",
      "epoch    2, loss: 0.0316, grapheme: 0.0016, vowel: 0.0064, consonant: 0.0064\n",
      "epoch    2, loss: 0.0306, grapheme: 0.0017, vowel: 0.0065, consonant: 0.0065\n",
      "epoch    2, loss: 0.0298, grapheme: 0.0018, vowel: 0.0065, consonant: 0.0065\n",
      "epoch    3, loss: 0.0282, grapheme: 0.0021, vowel: 0.0066, consonant: 0.0066\n",
      "epoch    3, loss: 0.0271, grapheme: 0.0022, vowel: 0.0067, consonant: 0.0066\n",
      "epoch    3, loss: 0.0272, grapheme: 0.0022, vowel: 0.0067, consonant: 0.0066\n",
      "epoch    3, loss: 0.0267, grapheme: 0.0023, vowel: 0.0066, consonant: 0.0066\n",
      "epoch    3, loss: 0.0261, grapheme: 0.0024, vowel: 0.0067, consonant: 0.0067\n",
      "epoch    3, loss: 0.0253, grapheme: 0.0025, vowel: 0.0067, consonant: 0.0067\n",
      "epoch    3, loss: 0.0247, grapheme: 0.0027, vowel: 0.0067, consonant: 0.0067\n",
      "epoch    3, loss: 0.0242, grapheme: 0.0027, vowel: 0.0068, consonant: 0.0068\n",
      "epoch    3, loss: 0.0240, grapheme: 0.0028, vowel: 0.0067, consonant: 0.0068\n",
      "epoch    3, loss: 0.0234, grapheme: 0.0029, vowel: 0.0068, consonant: 0.0068\n",
      "epoch    4, loss: 0.0222, grapheme: 0.0031, vowel: 0.0068, consonant: 0.0068\n",
      "epoch    4, loss: 0.0217, grapheme: 0.0031, vowel: 0.0069, consonant: 0.0068\n",
      "epoch    4, loss: 0.0215, grapheme: 0.0032, vowel: 0.0069, consonant: 0.0069\n",
      "epoch    4, loss: 0.0213, grapheme: 0.0032, vowel: 0.0069, consonant: 0.0069\n",
      "epoch    4, loss: 0.0206, grapheme: 0.0033, vowel: 0.0069, consonant: 0.0069\n",
      "epoch    4, loss: 0.0205, grapheme: 0.0034, vowel: 0.0069, consonant: 0.0069\n",
      "epoch    4, loss: 0.0206, grapheme: 0.0034, vowel: 0.0069, consonant: 0.0069\n",
      "epoch    4, loss: 0.0201, grapheme: 0.0035, vowel: 0.0069, consonant: 0.0069\n",
      "epoch    4, loss: 0.0201, grapheme: 0.0035, vowel: 0.0069, consonant: 0.0069\n",
      "epoch    4, loss: 0.0195, grapheme: 0.0036, vowel: 0.0070, consonant: 0.0070\n",
      "epoch    5, loss: 0.0182, grapheme: 0.0038, vowel: 0.0070, consonant: 0.0070\n",
      "epoch    5, loss: 0.0182, grapheme: 0.0038, vowel: 0.0070, consonant: 0.0071\n",
      "epoch    5, loss: 0.0182, grapheme: 0.0038, vowel: 0.0070, consonant: 0.0070\n",
      "epoch    5, loss: 0.0179, grapheme: 0.0039, vowel: 0.0070, consonant: 0.0070\n",
      "epoch    5, loss: 0.0180, grapheme: 0.0038, vowel: 0.0070, consonant: 0.0070\n",
      "epoch    5, loss: 0.0177, grapheme: 0.0040, vowel: 0.0070, consonant: 0.0071\n",
      "epoch    5, loss: 0.0178, grapheme: 0.0039, vowel: 0.0070, consonant: 0.0070\n",
      "epoch    5, loss: 0.0178, grapheme: 0.0040, vowel: 0.0070, consonant: 0.0070\n",
      "epoch    5, loss: 0.0174, grapheme: 0.0041, vowel: 0.0070, consonant: 0.0071\n",
      "epoch    5, loss: 0.0176, grapheme: 0.0040, vowel: 0.0070, consonant: 0.0070\n",
      "epoch    6, loss: 0.0159, grapheme: 0.0043, vowel: 0.0071, consonant: 0.0071\n",
      "epoch    6, loss: 0.0159, grapheme: 0.0043, vowel: 0.0071, consonant: 0.0071\n",
      "epoch    6, loss: 0.0160, grapheme: 0.0043, vowel: 0.0071, consonant: 0.0071\n",
      "epoch    6, loss: 0.0158, grapheme: 0.0043, vowel: 0.0071, consonant: 0.0071\n",
      "epoch    6, loss: 0.0158, grapheme: 0.0043, vowel: 0.0071, consonant: 0.0071\n",
      "epoch    6, loss: 0.0159, grapheme: 0.0044, vowel: 0.0071, consonant: 0.0071\n",
      "epoch    6, loss: 0.0155, grapheme: 0.0044, vowel: 0.0071, consonant: 0.0072\n",
      "epoch    6, loss: 0.0158, grapheme: 0.0043, vowel: 0.0071, consonant: 0.0071\n",
      "epoch    6, loss: 0.0155, grapheme: 0.0044, vowel: 0.0071, consonant: 0.0071\n",
      "epoch    6, loss: 0.0158, grapheme: 0.0043, vowel: 0.0071, consonant: 0.0072\n",
      "epoch    7, loss: 0.0139, grapheme: 0.0047, vowel: 0.0072, consonant: 0.0072\n",
      "epoch    7, loss: 0.0141, grapheme: 0.0047, vowel: 0.0072, consonant: 0.0072\n",
      "epoch    7, loss: 0.0142, grapheme: 0.0046, vowel: 0.0072, consonant: 0.0072\n",
      "epoch    7, loss: 0.0145, grapheme: 0.0046, vowel: 0.0072, consonant: 0.0072\n",
      "epoch    7, loss: 0.0144, grapheme: 0.0046, vowel: 0.0072, consonant: 0.0072\n",
      "epoch    7, loss: 0.0143, grapheme: 0.0047, vowel: 0.0072, consonant: 0.0072\n",
      "epoch    7, loss: 0.0141, grapheme: 0.0046, vowel: 0.0072, consonant: 0.0072\n",
      "epoch    7, loss: 0.0144, grapheme: 0.0046, vowel: 0.0072, consonant: 0.0072\n",
      "epoch    7, loss: 0.0144, grapheme: 0.0047, vowel: 0.0072, consonant: 0.0072\n",
      "epoch    7, loss: 0.0142, grapheme: 0.0046, vowel: 0.0072, consonant: 0.0072\n",
      "epoch    8, loss: 0.0129, grapheme: 0.0049, vowel: 0.0072, consonant: 0.0073\n",
      "epoch    8, loss: 0.0129, grapheme: 0.0049, vowel: 0.0073, consonant: 0.0072\n",
      "epoch    8, loss: 0.0130, grapheme: 0.0049, vowel: 0.0072, consonant: 0.0072\n",
      "epoch    8, loss: 0.0132, grapheme: 0.0049, vowel: 0.0072, consonant: 0.0072\n",
      "epoch    8, loss: 0.0132, grapheme: 0.0048, vowel: 0.0073, consonant: 0.0073\n",
      "epoch    8, loss: 0.0130, grapheme: 0.0049, vowel: 0.0072, consonant: 0.0073\n",
      "epoch    8, loss: 0.0132, grapheme: 0.0049, vowel: 0.0072, consonant: 0.0072\n",
      "epoch    8, loss: 0.0131, grapheme: 0.0049, vowel: 0.0072, consonant: 0.0073\n",
      "epoch    8, loss: 0.0132, grapheme: 0.0049, vowel: 0.0072, consonant: 0.0073\n",
      "epoch    8, loss: 0.0129, grapheme: 0.0050, vowel: 0.0073, consonant: 0.0072\n",
      "epoch    9, loss: 0.0117, grapheme: 0.0051, vowel: 0.0073, consonant: 0.0073\n",
      "epoch    9, loss: 0.0120, grapheme: 0.0051, vowel: 0.0073, consonant: 0.0073\n",
      "epoch    9, loss: 0.0120, grapheme: 0.0051, vowel: 0.0073, consonant: 0.0073\n",
      "epoch    9, loss: 0.0121, grapheme: 0.0051, vowel: 0.0073, consonant: 0.0073\n",
      "epoch    9, loss: 0.0124, grapheme: 0.0050, vowel: 0.0073, consonant: 0.0073\n",
      "epoch    9, loss: 0.0123, grapheme: 0.0051, vowel: 0.0073, consonant: 0.0073\n",
      "epoch    9, loss: 0.0122, grapheme: 0.0051, vowel: 0.0073, consonant: 0.0073\n",
      "epoch    9, loss: 0.0122, grapheme: 0.0051, vowel: 0.0073, consonant: 0.0073\n",
      "epoch    9, loss: 0.0121, grapheme: 0.0051, vowel: 0.0073, consonant: 0.0073\n",
      "epoch    9, loss: 0.0122, grapheme: 0.0050, vowel: 0.0073, consonant: 0.0073\n",
      "epoch   10, loss: 0.0109, grapheme: 0.0053, vowel: 0.0073, consonant: 0.0073\n",
      "epoch   10, loss: 0.0111, grapheme: 0.0053, vowel: 0.0073, consonant: 0.0073\n",
      "epoch   10, loss: 0.0110, grapheme: 0.0053, vowel: 0.0073, consonant: 0.0073\n",
      "epoch   10, loss: 0.0114, grapheme: 0.0052, vowel: 0.0073, consonant: 0.0073\n",
      "epoch   10, loss: 0.0116, grapheme: 0.0052, vowel: 0.0073, consonant: 0.0073\n",
      "epoch   10, loss: 0.0114, grapheme: 0.0052, vowel: 0.0073, consonant: 0.0073\n",
      "epoch   10, loss: 0.0113, grapheme: 0.0053, vowel: 0.0073, consonant: 0.0073\n",
      "epoch   10, loss: 0.0114, grapheme: 0.0053, vowel: 0.0073, consonant: 0.0073\n",
      "epoch   10, loss: 0.0115, grapheme: 0.0053, vowel: 0.0073, consonant: 0.0073\n",
      "epoch   10, loss: 0.0114, grapheme: 0.0052, vowel: 0.0073, consonant: 0.0073\n",
      "epoch   11, loss: 0.0103, grapheme: 0.0055, vowel: 0.0074, consonant: 0.0073\n",
      "epoch   11, loss: 0.0104, grapheme: 0.0054, vowel: 0.0074, consonant: 0.0074\n",
      "epoch   11, loss: 0.0103, grapheme: 0.0055, vowel: 0.0074, consonant: 0.0073\n",
      "epoch   11, loss: 0.0107, grapheme: 0.0054, vowel: 0.0073, consonant: 0.0073\n",
      "epoch   11, loss: 0.0106, grapheme: 0.0054, vowel: 0.0074, consonant: 0.0074\n",
      "epoch   11, loss: 0.0108, grapheme: 0.0054, vowel: 0.0073, consonant: 0.0074\n",
      "epoch   11, loss: 0.0106, grapheme: 0.0054, vowel: 0.0073, consonant: 0.0073\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch   11, loss: 0.0108, grapheme: 0.0054, vowel: 0.0073, consonant: 0.0073\n",
      "epoch   11, loss: 0.0107, grapheme: 0.0054, vowel: 0.0073, consonant: 0.0073\n",
      "epoch   11, loss: 0.0108, grapheme: 0.0054, vowel: 0.0073, consonant: 0.0073\n",
      "epoch   12, loss: 0.0095, grapheme: 0.0056, vowel: 0.0074, consonant: 0.0074\n",
      "epoch   12, loss: 0.0095, grapheme: 0.0056, vowel: 0.0074, consonant: 0.0074\n",
      "epoch   12, loss: 0.0102, grapheme: 0.0055, vowel: 0.0074, consonant: 0.0074\n",
      "epoch   12, loss: 0.0097, grapheme: 0.0056, vowel: 0.0074, consonant: 0.0074\n",
      "epoch   12, loss: 0.0102, grapheme: 0.0055, vowel: 0.0074, consonant: 0.0073\n",
      "epoch   12, loss: 0.0101, grapheme: 0.0055, vowel: 0.0074, consonant: 0.0074\n",
      "epoch   12, loss: 0.0102, grapheme: 0.0055, vowel: 0.0074, consonant: 0.0074\n",
      "epoch   12, loss: 0.0100, grapheme: 0.0055, vowel: 0.0074, consonant: 0.0074\n",
      "epoch   12, loss: 0.0103, grapheme: 0.0055, vowel: 0.0074, consonant: 0.0074\n",
      "epoch   12, loss: 0.0104, grapheme: 0.0055, vowel: 0.0074, consonant: 0.0073\n",
      "epoch   13, loss: 0.0088, grapheme: 0.0058, vowel: 0.0074, consonant: 0.0074\n",
      "epoch   13, loss: 0.0090, grapheme: 0.0058, vowel: 0.0074, consonant: 0.0074\n",
      "epoch   13, loss: 0.0091, grapheme: 0.0057, vowel: 0.0074, consonant: 0.0074\n",
      "epoch   13, loss: 0.0093, grapheme: 0.0056, vowel: 0.0074, consonant: 0.0074\n",
      "epoch   13, loss: 0.0093, grapheme: 0.0057, vowel: 0.0074, consonant: 0.0074\n",
      "epoch   13, loss: 0.0098, grapheme: 0.0056, vowel: 0.0074, consonant: 0.0074\n",
      "epoch   13, loss: 0.0094, grapheme: 0.0056, vowel: 0.0074, consonant: 0.0074\n",
      "epoch   13, loss: 0.0095, grapheme: 0.0056, vowel: 0.0074, consonant: 0.0074\n",
      "epoch   13, loss: 0.0098, grapheme: 0.0056, vowel: 0.0074, consonant: 0.0074\n",
      "epoch   13, loss: 0.0099, grapheme: 0.0055, vowel: 0.0074, consonant: 0.0073\n",
      "epoch   14, loss: 0.0087, grapheme: 0.0058, vowel: 0.0074, consonant: 0.0074\n",
      "epoch   14, loss: 0.0084, grapheme: 0.0059, vowel: 0.0074, consonant: 0.0074\n",
      "epoch   14, loss: 0.0086, grapheme: 0.0058, vowel: 0.0074, consonant: 0.0074\n",
      "epoch   14, loss: 0.0086, grapheme: 0.0059, vowel: 0.0074, consonant: 0.0074\n",
      "epoch   14, loss: 0.0091, grapheme: 0.0057, vowel: 0.0074, consonant: 0.0074\n",
      "epoch   14, loss: 0.0090, grapheme: 0.0058, vowel: 0.0074, consonant: 0.0074\n",
      "epoch   14, loss: 0.0090, grapheme: 0.0057, vowel: 0.0074, consonant: 0.0074\n",
      "epoch   14, loss: 0.0091, grapheme: 0.0057, vowel: 0.0074, consonant: 0.0074\n",
      "epoch   14, loss: 0.0093, grapheme: 0.0057, vowel: 0.0074, consonant: 0.0074\n",
      "epoch   14, loss: 0.0095, grapheme: 0.0056, vowel: 0.0074, consonant: 0.0074\n",
      "epoch   15, loss: 0.0079, grapheme: 0.0060, vowel: 0.0075, consonant: 0.0074\n",
      "epoch   15, loss: 0.0080, grapheme: 0.0059, vowel: 0.0075, consonant: 0.0075\n",
      "epoch   15, loss: 0.0083, grapheme: 0.0058, vowel: 0.0074, consonant: 0.0075\n",
      "epoch   15, loss: 0.0085, grapheme: 0.0059, vowel: 0.0075, consonant: 0.0074\n",
      "epoch   15, loss: 0.0087, grapheme: 0.0058, vowel: 0.0074, consonant: 0.0074\n",
      "epoch   15, loss: 0.0087, grapheme: 0.0058, vowel: 0.0074, consonant: 0.0074\n",
      "epoch   15, loss: 0.0087, grapheme: 0.0058, vowel: 0.0075, consonant: 0.0074\n",
      "epoch   15, loss: 0.0085, grapheme: 0.0059, vowel: 0.0074, consonant: 0.0074\n",
      "epoch   15, loss: 0.0089, grapheme: 0.0058, vowel: 0.0074, consonant: 0.0074\n",
      "epoch   15, loss: 0.0089, grapheme: 0.0057, vowel: 0.0074, consonant: 0.0074\n",
      "epoch   16, loss: 0.0076, grapheme: 0.0060, vowel: 0.0075, consonant: 0.0075\n",
      "epoch   16, loss: 0.0077, grapheme: 0.0060, vowel: 0.0075, consonant: 0.0075\n",
      "epoch   16, loss: 0.0079, grapheme: 0.0059, vowel: 0.0075, consonant: 0.0075\n",
      "epoch   16, loss: 0.0080, grapheme: 0.0059, vowel: 0.0075, consonant: 0.0075\n",
      "epoch   16, loss: 0.0079, grapheme: 0.0059, vowel: 0.0075, consonant: 0.0075\n",
      "epoch   16, loss: 0.0083, grapheme: 0.0059, vowel: 0.0075, consonant: 0.0074\n",
      "epoch   16, loss: 0.0082, grapheme: 0.0059, vowel: 0.0075, consonant: 0.0074\n",
      "epoch   16, loss: 0.0086, grapheme: 0.0059, vowel: 0.0074, consonant: 0.0074\n",
      "epoch   16, loss: 0.0083, grapheme: 0.0059, vowel: 0.0074, consonant: 0.0074\n",
      "epoch   16, loss: 0.0085, grapheme: 0.0059, vowel: 0.0074, consonant: 0.0074\n",
      "epoch   17, loss: 0.0073, grapheme: 0.0061, vowel: 0.0075, consonant: 0.0075\n",
      "epoch   17, loss: 0.0072, grapheme: 0.0061, vowel: 0.0075, consonant: 0.0075\n",
      "epoch   17, loss: 0.0073, grapheme: 0.0061, vowel: 0.0075, consonant: 0.0075\n",
      "epoch   17, loss: 0.0076, grapheme: 0.0060, vowel: 0.0075, consonant: 0.0075\n",
      "epoch   17, loss: 0.0076, grapheme: 0.0061, vowel: 0.0075, consonant: 0.0075\n",
      "epoch   17, loss: 0.0079, grapheme: 0.0060, vowel: 0.0075, consonant: 0.0075\n",
      "epoch   17, loss: 0.0079, grapheme: 0.0060, vowel: 0.0075, consonant: 0.0075\n",
      "epoch   17, loss: 0.0080, grapheme: 0.0059, vowel: 0.0075, consonant: 0.0074\n",
      "epoch   17, loss: 0.0082, grapheme: 0.0059, vowel: 0.0074, consonant: 0.0074\n",
      "epoch   17, loss: 0.0081, grapheme: 0.0059, vowel: 0.0075, consonant: 0.0074\n",
      "epoch   18, loss: 0.0070, grapheme: 0.0062, vowel: 0.0075, consonant: 0.0075\n",
      "epoch   18, loss: 0.0071, grapheme: 0.0061, vowel: 0.0075, consonant: 0.0075\n",
      "epoch   18, loss: 0.0073, grapheme: 0.0061, vowel: 0.0075, consonant: 0.0075\n",
      "epoch   18, loss: 0.0071, grapheme: 0.0062, vowel: 0.0075, consonant: 0.0075\n",
      "epoch   18, loss: 0.0073, grapheme: 0.0061, vowel: 0.0075, consonant: 0.0075\n",
      "epoch   18, loss: 0.0075, grapheme: 0.0060, vowel: 0.0075, consonant: 0.0075\n",
      "epoch   18, loss: 0.0076, grapheme: 0.0060, vowel: 0.0075, consonant: 0.0075\n",
      "epoch   18, loss: 0.0077, grapheme: 0.0060, vowel: 0.0075, consonant: 0.0075\n",
      "epoch   18, loss: 0.0077, grapheme: 0.0060, vowel: 0.0075, consonant: 0.0075\n",
      "epoch   18, loss: 0.0076, grapheme: 0.0061, vowel: 0.0075, consonant: 0.0075\n",
      "epoch   19, loss: 0.0067, grapheme: 0.0062, vowel: 0.0075, consonant: 0.0075\n",
      "epoch   19, loss: 0.0067, grapheme: 0.0062, vowel: 0.0075, consonant: 0.0075\n",
      "epoch   19, loss: 0.0071, grapheme: 0.0061, vowel: 0.0075, consonant: 0.0075\n",
      "epoch   19, loss: 0.0069, grapheme: 0.0062, vowel: 0.0075, consonant: 0.0075\n",
      "epoch   19, loss: 0.0071, grapheme: 0.0062, vowel: 0.0075, consonant: 0.0075\n",
      "epoch   19, loss: 0.0072, grapheme: 0.0061, vowel: 0.0075, consonant: 0.0075\n",
      "epoch   19, loss: 0.0072, grapheme: 0.0061, vowel: 0.0075, consonant: 0.0075\n",
      "epoch   19, loss: 0.0072, grapheme: 0.0061, vowel: 0.0075, consonant: 0.0075\n",
      "epoch   19, loss: 0.0074, grapheme: 0.0061, vowel: 0.0075, consonant: 0.0075\n",
      "epoch   19, loss: 0.0073, grapheme: 0.0061, vowel: 0.0075, consonant: 0.0075\n",
      "epoch   20, loss: 0.0063, grapheme: 0.0063, vowel: 0.0075, consonant: 0.0075\n",
      "epoch   20, loss: 0.0063, grapheme: 0.0063, vowel: 0.0075, consonant: 0.0075\n",
      "epoch   20, loss: 0.0067, grapheme: 0.0062, vowel: 0.0075, consonant: 0.0075\n",
      "epoch   20, loss: 0.0067, grapheme: 0.0063, vowel: 0.0075, consonant: 0.0075\n",
      "epoch   20, loss: 0.0068, grapheme: 0.0062, vowel: 0.0075, consonant: 0.0075\n",
      "epoch   20, loss: 0.0069, grapheme: 0.0062, vowel: 0.0075, consonant: 0.0075\n",
      "epoch   20, loss: 0.0069, grapheme: 0.0062, vowel: 0.0075, consonant: 0.0075\n",
      "epoch   20, loss: 0.0069, grapheme: 0.0062, vowel: 0.0075, consonant: 0.0075\n",
      "epoch   20, loss: 0.0071, grapheme: 0.0061, vowel: 0.0075, consonant: 0.0075\n",
      "epoch   20, loss: 0.0072, grapheme: 0.0061, vowel: 0.0075, consonant: 0.0075\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = next(iter(trainloader))\n",
    "output = model(samples['image'].to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8046875, 0.9765625, 0.9609375)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_metrics(output, samples[\"labels\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"multilabel_mobilenet_class_fixed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/train-test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>image_id</th>\n",
       "      <th>grapheme_root</th>\n",
       "      <th>vowel_diacritic</th>\n",
       "      <th>consonant_diacritic</th>\n",
       "      <th>grapheme</th>\n",
       "      <th>label</th>\n",
       "      <th>type</th>\n",
       "      <th>textlabel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Train_0</td>\n",
       "      <td>15</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>ক্ট্রো</td>\n",
       "      <td>64</td>\n",
       "      <td>test</td>\n",
       "      <td>test\\ngr=15 vd=9 cd=5\\n64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Train_1</td>\n",
       "      <td>159</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>হ</td>\n",
       "      <td>1243</td>\n",
       "      <td>train</td>\n",
       "      <td>train\\ngr=159 vd=0 cd=0\\n1243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Train_2</td>\n",
       "      <td>22</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>খ্রী</td>\n",
       "      <td>107</td>\n",
       "      <td>train</td>\n",
       "      <td>train\\ngr=22 vd=3 cd=5\\n107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Train_3</td>\n",
       "      <td>53</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>র্টি</td>\n",
       "      <td>333</td>\n",
       "      <td>train</td>\n",
       "      <td>train\\ngr=53 vd=2 cd=2\\n333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Train_4</td>\n",
       "      <td>71</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>থ্রো</td>\n",
       "      <td>504</td>\n",
       "      <td>test</td>\n",
       "      <td>test\\ngr=71 vd=9 cd=5\\n504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200835</th>\n",
       "      <td>200835</td>\n",
       "      <td>Train_200835</td>\n",
       "      <td>22</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>র্খে</td>\n",
       "      <td>112</td>\n",
       "      <td>train</td>\n",
       "      <td>train\\ngr=22 vd=7 cd=2\\n112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200836</th>\n",
       "      <td>200836</td>\n",
       "      <td>Train_200836</td>\n",
       "      <td>65</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>ত্তো</td>\n",
       "      <td>462</td>\n",
       "      <td>test</td>\n",
       "      <td>test\\ngr=65 vd=9 cd=0\\n462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200837</th>\n",
       "      <td>200837</td>\n",
       "      <td>Train_200837</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>অ্যা</td>\n",
       "      <td>3</td>\n",
       "      <td>test</td>\n",
       "      <td>test\\ngr=2 vd=1 cd=4\\n3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200838</th>\n",
       "      <td>200838</td>\n",
       "      <td>Train_200838</td>\n",
       "      <td>152</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>স্নো</td>\n",
       "      <td>1213</td>\n",
       "      <td>test</td>\n",
       "      <td>test\\ngr=152 vd=9 cd=0\\n1213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200839</th>\n",
       "      <td>200839</td>\n",
       "      <td>Train_200839</td>\n",
       "      <td>127</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>ল্টি</td>\n",
       "      <td>1000</td>\n",
       "      <td>train</td>\n",
       "      <td>train\\ngr=127 vd=2 cd=0\\n1000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200840 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0      image_id  grapheme_root  vowel_diacritic  \\\n",
       "0                0       Train_0             15                9   \n",
       "1                1       Train_1            159                0   \n",
       "2                2       Train_2             22                3   \n",
       "3                3       Train_3             53                2   \n",
       "4                4       Train_4             71                9   \n",
       "...            ...           ...            ...              ...   \n",
       "200835      200835  Train_200835             22                7   \n",
       "200836      200836  Train_200836             65                9   \n",
       "200837      200837  Train_200837              2                1   \n",
       "200838      200838  Train_200838            152                9   \n",
       "200839      200839  Train_200839            127                2   \n",
       "\n",
       "        consonant_diacritic grapheme  label   type  \\\n",
       "0                         5   ক্ট্রো     64   test   \n",
       "1                         0        হ   1243  train   \n",
       "2                         5     খ্রী    107  train   \n",
       "3                         2     র্টি    333  train   \n",
       "4                         5     থ্রো    504   test   \n",
       "...                     ...      ...    ...    ...   \n",
       "200835                    2     র্খে    112  train   \n",
       "200836                    0     ত্তো    462   test   \n",
       "200837                    4     অ্যা      3   test   \n",
       "200838                    0     স্নো   1213   test   \n",
       "200839                    0     ল্টি   1000  train   \n",
       "\n",
       "                            textlabel  \n",
       "0           test\\ngr=15 vd=9 cd=5\\n64  \n",
       "1       train\\ngr=159 vd=0 cd=0\\n1243  \n",
       "2         train\\ngr=22 vd=3 cd=5\\n107  \n",
       "3         train\\ngr=53 vd=2 cd=2\\n333  \n",
       "4          test\\ngr=71 vd=9 cd=5\\n504  \n",
       "...                               ...  \n",
       "200835    train\\ngr=22 vd=7 cd=2\\n112  \n",
       "200836     test\\ngr=65 vd=9 cd=0\\n462  \n",
       "200837        test\\ngr=2 vd=1 cd=4\\n3  \n",
       "200838   test\\ngr=152 vd=9 cd=0\\n1213  \n",
       "200839  train\\ngr=127 vd=2 cd=0\\n1000  \n",
       "\n",
       "[200840 rows x 9 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "167"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.grapheme_root.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.vowel_diacritic.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.consonant_diacritic.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
