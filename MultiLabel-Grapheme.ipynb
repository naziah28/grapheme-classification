{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# from model.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "from PIL import Image\n",
    "import pandas as pd \n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "class MultiOutputModel(nn.Module):\n",
    "    def __init__(self, n_grapheme_classes, n_vowel_classes, n_consonant_classes):\n",
    "        super().__init__()\n",
    "        self.base_model = models.mobilenet_v2().features  # take the model without classifier\n",
    "        self.base_model[0][0] = nn.Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
    "\n",
    "        last_channel = models.mobilenet_v2().last_channel  # size of the layer before classifier\n",
    "\n",
    "        # the input for the classifier should be two-dimensional, but we will have\n",
    "        # [batch_size, channels, width, height]\n",
    "        # so, let's do the spatial averaging: reduce width and height to 1\n",
    "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "\n",
    "        # create separate classifiers for our outputs\n",
    "        self.grapheme = nn.Sequential(\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(in_features=last_channel, out_features=n_grapheme_classes)\n",
    "        )\n",
    "        self.vowel = nn.Sequential(\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(in_features=last_channel, out_features=n_vowel_classes)\n",
    "        )\n",
    "        self.consonant = nn.Sequential(\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(in_features=last_channel, out_features=n_consonant_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.base_model(x)\n",
    "        x = self.pool(x)\n",
    "\n",
    "        # reshape from [batch, channels, 1, 1] to [batch, channels] to put it into classifier\n",
    "        x = torch.flatten(x, 1)\n",
    "\n",
    "        return {\n",
    "            'grapheme': self.grapheme(x),\n",
    "            'vowel': self.vowel(x),\n",
    "            'consonant': self.consonant(x)\n",
    "        }\n",
    "\n",
    "    def get_loss(self, net_output, ground_truth):\n",
    "        color_loss = F.cross_entropy(net_output['grapheme'], ground_truth['grapheme_labels'])\n",
    "        gender_loss = F.cross_entropy(net_output['vowel'], ground_truth['vowel_labels'])\n",
    "        article_loss = F.cross_entropy(net_output['consonant'], ground_truth['consonant_labels'])\n",
    "        loss = color_loss + gender_loss + article_loss\n",
    "        return loss, {'grapheme': color_loss, 'vowel': gender_loss, 'consonant': article_loss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BengaliDatasetMultiClass(Dataset):\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.label_df = pd.read_csv(csv_file)\n",
    "        self.label_df = self.label_df[['image_id','grapheme_root',\n",
    "                             'vowel_diacritic','consonant_diacritic',\n",
    "                             'label','grapheme','textlabel']]\n",
    "        \n",
    "        self.root_dir = root_dir \n",
    "        self.transform = transform \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.label_df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "            \n",
    "        img_name = os.path.join(self.root_dir,\n",
    "                                self.label_df.iloc[idx, 0] + '.png')\n",
    "        image = Image.open(img_name).convert('L')\n",
    "\n",
    "        label = tuple(self.label_df.iloc[idx, 1:])\n",
    "#         label = torch.tensor(label)\n",
    "        textlabel = self.label_df.iloc[idx, -1]  \n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        sample = {\n",
    "            \"image\": image,\n",
    "            \"labels\": {\n",
    "                \"grapheme_labels\": label[0],\n",
    "                \"vowel_labels\": label[1],\n",
    "                \"consonant_labels\": label[2],\n",
    "            },\n",
    "            \"human_labels\":{\n",
    "                \"typeface\":label[4],\n",
    "                \"stringlabel\":label[5]\n",
    "            }\n",
    "        }\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=128\n",
    "transform=transforms.Compose([\n",
    "    transforms.Resize(28),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "trainset = BengaliDatasetMultiClass(\"data/train.csv\",\"data/trainsplit\", transform)\n",
    "testset = BengaliDatasetMultiClass(\"data/test.csv\",\"data/testsplit\", transform)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                         shuffle=True, num_workers=16)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=True, num_workers=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = next(iter(trainloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultiOutputModel(168, 11,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score\n",
    "\n",
    "def calculate_metrics(output, target):\n",
    "    _, predicted_color = output['grapheme'].cpu().max(1)\n",
    "    gt_color = target['grapheme_labels'].cpu()\n",
    "\n",
    "    _, predicted_gender = output['vowel'].cpu().max(1)\n",
    "    gt_gender = target['vowel_labels'].cpu()\n",
    "\n",
    "    _, predicted_article = output['consonant'].cpu().max(1)\n",
    "    gt_article = target['consonant_labels'].cpu()\n",
    "\n",
    "    with warnings.catch_warnings():  # sklearn may produce a warning when processing zero row in confusion matrix\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        accuracy_color = accuracy_score(y_true=gt_color.numpy(), y_pred=predicted_color.numpy())\n",
    "        accuracy_gender = accuracy_score(y_true=gt_gender.numpy(), y_pred=predicted_gender.numpy())\n",
    "        accuracy_article = accuracy_score(y_true=gt_article.numpy(), y_pred=predicted_article.numpy())\n",
    "\n",
    "    return accuracy_color, accuracy_gender, accuracy_article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from train.py\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "def get_cur_time():\n",
    "    return datetime.strftime(datetime.now(), '%Y-%m-%d_%H-%M')\n",
    "\n",
    "\n",
    "def checkpoint_save(model, name, epoch):\n",
    "    f = os.path.join(name, 'checkpoint-{:06d}.pth'.format(epoch))\n",
    "    torch.save(model.state_dict(), f)\n",
    "    print('Saved checkpoint:', f)\n",
    "\n",
    "    return f\n",
    "    \n",
    "\n",
    "def train(start_epoch=1, N_epochs=1, batch_size=16, num_workers=8):\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "\n",
    "    for epoch in range(start_epoch, N_epochs + 1):\n",
    "        total_loss = 0\n",
    "        accuracy_color = 0\n",
    "        accuracy_gender = 0\n",
    "        accuracy_article = 0\n",
    "\n",
    "        for i, batch in enumerate(trainloader):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            img = batch['image']\n",
    "            target_labels = batch['labels']\n",
    "            target_labels = {t: target_labels[t].to(device) for t in target_labels}\n",
    "            output = model(img.to(device))\n",
    "\n",
    "            loss_train, losses_train = model.get_loss(output, target_labels)\n",
    "            total_loss += loss_train.item()\n",
    "            batch_accuracy_color, batch_accuracy_gender, batch_accuracy_article = \\\n",
    "                calculate_metrics(output, target_labels)\n",
    "\n",
    "            accuracy_color += batch_accuracy_color\n",
    "            accuracy_gender += batch_accuracy_gender\n",
    "            accuracy_article += batch_accuracy_article\n",
    "\n",
    "            loss_train.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            if i%100 == 0: \n",
    "                print(\"epoch {:4d}, loss: {:.4f}, grapheme: {:.4f}, vowel: {:.4f}, consonant: {:.4f}\".format(\n",
    "                    epoch,\n",
    "                    total_loss / len(trainset),\n",
    "                    accuracy_color / len(trainset),\n",
    "                    accuracy_gender / len(trainset),\n",
    "                    accuracy_article / len(trainset)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch    1, loss: 0.0001, grapheme: 0.0000, vowel: 0.0000, consonant: 0.0000\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = next(iter(trainloader))\n",
    "output = model(samples['image'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_metrics(output, samples[\"labels\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
