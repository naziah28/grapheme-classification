{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from model.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "from PIL import Image\n",
    "import pandas as pd \n",
    "\n",
    "class MultiOutputModel(nn.Module):\n",
    "    def __init__(self, n_grapheme_classes, n_vowel_classes, n_consonant_classes):\n",
    "        super().__init__()\n",
    "        self.base_model = models.mobilenet_v2().features  # take the model without classifier\n",
    "        self.base_model[0][0] = nn.Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
    "\n",
    "        last_channel = models.mobilenet_v2().last_channel  # size of the layer before classifier\n",
    "\n",
    "        # the input for the classifier should be two-dimensional, but we will have\n",
    "        # [batch_size, channels, width, height]\n",
    "        # so, let's do the spatial averaging: reduce width and height to 1\n",
    "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "\n",
    "        # create separate classifiers for our outputs\n",
    "        self.grapheme = nn.Sequential(\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(in_features=last_channel, out_features=n_grapheme_classes)\n",
    "        )\n",
    "        self.vowel = nn.Sequential(\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(in_features=last_channel, out_features=n_vowel_classes)\n",
    "        )\n",
    "        self.consonant = nn.Sequential(\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(in_features=last_channel, out_features=n_consonant_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.base_model(x)\n",
    "        x = self.pool(x)\n",
    "\n",
    "        # reshape from [batch, channels, 1, 1] to [batch, channels] to put it into classifier\n",
    "        x = torch.flatten(x, 1)\n",
    "\n",
    "        return {\n",
    "            'grapheme': self.grapheme(x),\n",
    "            'vowel': self.vowel(x),\n",
    "            'consonant': self.consonant(x)\n",
    "        }\n",
    "\n",
    "    def get_loss(self, net_output, ground_truth):\n",
    "        color_loss = F.cross_entropy(net_output['grapheme'], ground_truth['grapheme_labels'])\n",
    "        gender_loss = F.cross_entropy(net_output['vowel'], ground_truth['vowel_labels'])\n",
    "        article_loss = F.cross_entropy(net_output['consonant'], ground_truth['consonant_labels'])\n",
    "        loss = color_loss + gender_loss + article_loss\n",
    "        return loss, {'grapheme': color_loss, 'vowel': gender_loss, 'consonant': article_loss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = models.mobilenet_v2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BengaliDatasetMultiClass(Dataset):\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.label_df = pd.read_csv(csv_file)\n",
    "        self.label_df = self.label_df[['image_id','grapheme_root',\n",
    "                             'vowel_diacritic','consonant_diacritic',\n",
    "                             'label','grapheme','textlabel']]\n",
    "        \n",
    "        self.root_dir = root_dir \n",
    "        self.transform = transform \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.label_df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "            \n",
    "        img_name = os.path.join(self.root_dir,\n",
    "                                self.label_df.iloc[idx, 0] + '.png')\n",
    "        image = Image.open(img_name).convert('L')\n",
    "\n",
    "        label = tuple(self.label_df.iloc[idx, 1:4])\n",
    "        label = torch.tensor(label)\n",
    "        textlabel = self.label_df.iloc[idx, -1]  \n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        sample = {\n",
    "            \"image\": image, \n",
    "            \"grapheme_root\": label[0],\n",
    "            \"vowel_diacritic\": label[1],\n",
    "            \"consonent_diacritic\": label[2]\n",
    "        }\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=4\n",
    "transform=transforms.Compose([\n",
    "    transforms.Resize(28),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "trainset = BengaliDatasetMultiClass(\"data/train.csv\",\"data/trainsplit\", transform)\n",
    "testset = BengaliDatasetMultiClass(\"data/test.csv\",\"data/testsplit\", transform)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                         shuffle=True, num_workers=0)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=True, num_workers=0)\n",
    "\n",
    "# df = pd.read_csv(\"data/train.csv\")\n",
    "# classes = list(range(df.label.max()+1))\n",
    "# n_classes = len(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = next(iter(trainloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = MultiOutputModel(168, 11,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 168])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net(samples[\"image\"])[\"grapheme\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
